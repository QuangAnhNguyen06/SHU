{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "#from func import *\n",
    "import func\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "folder_path = \"F:/code/barlow/UR5\"\n",
    "file_pairs = func.load_file_pairs(folder_path)\n",
    "\n",
    "images = []\n",
    "npy_file = []\n",
    "for jpg_path, npy_path in file_pairs:\n",
    "    images.append(func.read_jpg_files(jpg_path))\n",
    "    npy_file.append(func.read_and_parse_npy_file(npy_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "CROP_TO = 32\n",
    "SEED = 42\n",
    "\n",
    "PROJECT_DIM = 2048\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Functions\n",
    "\n",
    "def augment_image(image, crop_ratio=0.05):\n",
    "    h, w = image.shape\n",
    "    # Calculate crop dimensions\n",
    "    crop_h = int(h * crop_ratio)\n",
    "    crop_w = int(w * crop_ratio)\n",
    "\n",
    "    # cropping\n",
    "    cropped_image = image[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "\n",
    "    # Resize back to original\n",
    "    resized_image = cv2.resize(cropped_image, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "# No2. Augmentation Functions\n",
    "def adjust_brightness(image, factor=1.1):\n",
    "    \"\"\"\n",
    "    scale pixel values\n",
    "    \"\"\"\n",
    "    adjusted = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
    "    return adjusted\n",
    "\n",
    "def add_noise_vector(vector,alpha= 0.01):\n",
    "    noise = np.random.normal(0, alpha, size=len(vector)) # alpha standard deviation\n",
    "    augmented_vector = [v + n for v, n in zip(vector, noise)]\n",
    "    #augmented_vector = [i.astype(float) for i in augmented_vector]\n",
    "    return augmented_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 597 // BATCH_SIZE\n",
    "TOTAL_STEPS = STEPS_PER_EPOCH * EPOCHS\n",
    "WARMUP_EPOCHS = int(EPOCHS * 0.1)\n",
    "WARMUP_STEPS = int(WARMUP_EPOCHS * STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train loss data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# image\n",
    "image1 = [augment_image(img) for img in images]\n",
    "image2 = [adjust_brightness(img, factor=1.2) for img in images]\n",
    "\n",
    "# noise\n",
    "npy_file_1 = [add_noise_vector(i,alpha= 0.05) for i in npy_file]\n",
    "npy_file_2 = [add_noise_vector(i,alpha= 0.10) for i in npy_file]\n",
    "\n",
    "# Normalize\n",
    "image1 = [(i.astype(\"float32\") / 255.0) for i in image1]\n",
    "image2 = [(i.astype(\"float32\") / 255.0) for i in image2]\n",
    "npy_file_1 = scaler.fit_transform(npy_file_1)\n",
    "npy_file_2 = scaler.fit_transform(npy_file_2)\n",
    "\n",
    "\n",
    "img_ds_1 = tf.data.Dataset.from_tensor_slices(image1).batch(BATCH_SIZE)\n",
    "img_ds_2 = tf.data.Dataset.from_tensor_slices(image2).batch(BATCH_SIZE)\n",
    "npy_ds_1 = tf.data.Dataset.from_tensor_slices(npy_file_1).batch(BATCH_SIZE)\n",
    "npy_ds_2 = tf.data.Dataset.from_tensor_slices(npy_file_2).batch(BATCH_SIZE)\n",
    "\n",
    "# Combine image và vector thành input pairs\n",
    "BL_ds = tf.data.Dataset.zip((img_ds_1, img_ds_2, npy_ds_1, npy_ds_2)).prefetch(tf.data.AUTOTUNE)\n",
    "BL_ds_test = BL_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 shape: (32, 128, 128)\n",
      "Image 2 shape: (32, 128, 128)\n",
      "Vector 1 shape: (32, 8)\n",
      "Vector 2 shape: (32, 8)\n"
     ]
    }
   ],
   "source": [
    "for batch in BL_ds_test.take(1):\n",
    "    (x1, x2, y1, y2) = batch\n",
    "    print(\"Image 1 shape:\", x1.shape)  \n",
    "    print(\"Image 2 shape:\", x2.shape)  \n",
    "    print(\"Vector 1 shape:\", y1.shape)  \n",
    "    print(\"Vector 2 shape:\", y2.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barlow Twin Loss\n",
    "def off_diagonal(x):\n",
    "    n = tf.shape(x)[0]\n",
    "    flattened = tf.reshape(x, [-1])[:-1]\n",
    "    off_diagonals = tf.reshape(flattened, (n - 1, n + 1))[:, 1:]\n",
    "    return tf.reshape(off_diagonals, [-1])\n",
    "\n",
    "def normalize_repr(z):\n",
    "    z_norm = (z - tf.reduce_mean(z, axis=0)) / (tf.math.reduce_std(z, axis=0) + 1e-8)\n",
    "    return z_norm\n",
    "\n",
    "def compute_loss(z_a, z_b, lambd = 5e-3):\n",
    "    batch_size = tf.cast(tf.shape(z_a)[0], z_a.dtype)\n",
    "    repr_dim = tf.shape(z_a)[1]\n",
    "\n",
    "    # Normalize the representations along the batch dimension.\n",
    "    z_a_norm = normalize_repr(z_a)\n",
    "    z_b_norm = normalize_repr(z_b)\n",
    "\n",
    "    # Cross-correlation matrix.\n",
    "    c = tf.matmul(z_a_norm, z_b_norm, transpose_a=True) / batch_size\n",
    "\n",
    "    # Loss.\n",
    "    on_diag = tf.linalg.diag_part(c) + (-1)\n",
    "    on_diag = tf.reduce_sum(tf.pow(on_diag, 2))\n",
    "    off_diag = off_diagonal(c)\n",
    "    off_diag = tf.reduce_sum(tf.pow(off_diag, 2))\n",
    "    loss = on_diag + (lambd * off_diag)\n",
    "    return loss   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_1():\n",
    "    inputs = layers.Input(shape=(128, 128, 1), name=\"image_input\") \n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(30, activation=\"relu\")(x)  # Image embedding\n",
    "    return tf.keras.Model(inputs, outputs, name=\"ImageNetwork\")\n",
    "\n",
    "def network_2():\n",
    "    inputs = layers.Input(shape=(8,), name=\"npy_input\")\n",
    "    outputs = tf.keras.layers.Dense(10, activation=\"relu\")(inputs)  # Joint embedding\n",
    "    return tf.keras.Model(inputs, outputs, name=\"JointStateNetwork\")\n",
    "\n",
    "\n",
    "def combined_model():\n",
    "    image_input = layers.Input(shape=(128, 128, 1), name=\"image_input\")\n",
    "    npy_input = layers.Input(shape=(8,), name=\"npy_input\")\n",
    "    \n",
    "    image_output = network_1()(image_input)\n",
    "    npy_output = network_2()(npy_input)\n",
    "    \n",
    "    combined = layers.Concatenate()([image_output, npy_output])\n",
    "    latent_space = layers.Dense(20, activation=\"relu\", name=\"latent_space\")(combined)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[image_input, npy_input], outputs=latent_space, name=\"CombinedModel\")\n",
    "\n",
    "@tf.function\n",
    "def train_step(image1, image2, npy1, npy2, network1, network2, optimizer, lambda1, lambda2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass for images\n",
    "        z_img1 = network1(image1, training=True)\n",
    "        z_img2 = network1(image2, training=True)\n",
    "        loss1 = compute_loss(z_img1, z_img2)\n",
    "\n",
    "        # Forward pass for joints\n",
    "        z_npy1 = network2(npy1, training=True)\n",
    "        z_npy2 = network2(npy2, training=True)\n",
    "        loss2 = compute_loss(z_npy1, z_npy2)\n",
    "\n",
    "        # Total loss\n",
    "        # with lambda = 1/Dimension\n",
    "        total_loss = loss1 * lambda1 + loss2 * lambda2\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, network1.trainable_variables + network2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, network1.trainable_variables + network2.trainable_variables))\n",
    "    \n",
    "    return {\"loss_total\": total_loss, \"loss1\": loss1, \"loss2\": loss2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Loss: 10.6736, Loss1: 42.1321, Loss2: 8.0403\n",
      "Epoch 2, Total Loss: 8.9237, Loss1: 16.4785, Loss2: 7.8938\n",
      "Epoch 3, Total Loss: 8.5998, Loss1: 15.5495, Loss2: 7.6279\n",
      "Epoch 4, Total Loss: 8.0723, Loss1: 12.8648, Loss2: 7.2683\n",
      "Epoch 5, Total Loss: 7.6492, Loss1: 11.7760, Loss2: 6.9132\n",
      "Epoch 6, Total Loss: 7.3995, Loss1: 12.0388, Loss2: 6.6471\n",
      "Epoch 7, Total Loss: 7.2920, Loss1: 12.9254, Loss2: 6.4841\n",
      "Epoch 8, Total Loss: 7.2225, Loss1: 11.8166, Loss2: 6.4840\n",
      "Epoch 9, Total Loss: 7.3166, Loss1: 13.3224, Loss2: 6.4839\n",
      "Epoch 10, Total Loss: 7.2230, Loss1: 11.8260, Loss2: 6.4839\n",
      "Epoch 11, Total Loss: 7.2197, Loss1: 11.7726, Loss2: 6.4839\n",
      "Epoch 12, Total Loss: 7.2756, Loss1: 12.6673, Loss2: 6.4839\n",
      "Epoch 13, Total Loss: 7.2228, Loss1: 11.8222, Loss2: 6.4839\n",
      "Epoch 14, Total Loss: 7.2778, Loss1: 12.7019, Loss2: 6.4839\n",
      "Epoch 15, Total Loss: 7.2844, Loss1: 12.8075, Loss2: 6.4839\n",
      "Epoch 16, Total Loss: 7.2224, Loss1: 11.8156, Loss2: 6.4839\n",
      "Epoch 17, Total Loss: 7.2293, Loss1: 11.9254, Loss2: 6.4839\n",
      "Epoch 18, Total Loss: 7.2934, Loss1: 12.9518, Loss2: 6.4839\n",
      "Epoch 19, Total Loss: 7.2250, Loss1: 11.8572, Loss2: 6.4839\n",
      "Epoch 20, Total Loss: 7.2337, Loss1: 11.9969, Loss2: 6.4839\n"
     ]
    }
   ],
   "source": [
    "lr_decayed_fn = lr_scheduler.WarmUpCosine(\n",
    "    learning_rate_base=1e-4,\n",
    "    total_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=WARMUP_STEPS\n",
    ")\n",
    "\n",
    "#lr_decayed_fn = 5e-4     # 0.005 = 5e-3\n",
    "\n",
    "model = combined_model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9)\n",
    "lambd = 5e-3 \n",
    "EPOCHS = 20\n",
    "lambda1 = 1/16\n",
    "lambda2 = 1\n",
    "network1 = network_1()\n",
    "network2 = network_2()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for image1_batch, image2_batch, npy1_batch, npy2_batch in BL_ds_test:\n",
    "        losses = train_step(image1_batch, image2_batch, npy1_batch, npy2_batch, network1, network2, optimizer, lambda1, lambda2)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Total Loss: {losses['loss_total']:.4f}, Loss1: {losses['loss1']:.4f}, Loss2: {losses['loss2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func\n",
    "def process_data(data_list):\n",
    "    a = []\n",
    "    b = []\n",
    "    for jpg_path, npy_path in data_list:\n",
    "        a.append(func.read_jpg_files(jpg_path))\n",
    "        b.append(func.read_and_parse_npy_file(npy_path))\n",
    "    return a,b\n",
    "\n",
    "# create train test dataset\n",
    "train_similar, train_dissimilar, test_similar, test_dissimilar = func.split_and_shuffle_pairs(file_pairs, folder_path)\n",
    "\n",
    "xtrain1 = process_data(train_similar)\n",
    "xtrain2 = process_data(train_dissimilar)\n",
    "xtest1 = process_data(test_similar)\n",
    "xtest2 = process_data(test_dissimilar)\n",
    "\n",
    "# label 1 - similar, 0 - disimilar\n",
    "ytrain1 = [1] * 500\n",
    "ytrain2 = [0] * 500\n",
    "ytest1 = [1] *79\n",
    "ytest2 = [0] *79\n",
    "\n",
    "#\n",
    "t1 = np.array([np.expand_dims(item, axis=-1) for item in xtrain1[0]])  # Expand dims\n",
    "t1 = t1.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t2 = np.array([item for item in xtrain1[1]])  # NPY input\n",
    "t2 = scaler.fit_transform(t2)  # Standardize to mean=0, std=1\n",
    "train_ds1 = tf.data.Dataset.from_tensor_slices(((t1, t2), ytrain1))\n",
    "\n",
    "#\n",
    "t3 = np.array([np.expand_dims(item, axis=-1) for item in xtrain2[0]])  # Expand dims\n",
    "t3 = t3.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t4 = np.array([item for item in xtrain2[1]])  # NPY input\n",
    "t4 = scaler.transform(t4)  # Sử dụng scaler đã fit từ trước\n",
    "train_ds2 = tf.data.Dataset.from_tensor_slices(((t3, t4), ytrain2))\n",
    "\n",
    "\n",
    "# concat\n",
    "train_ds = train_ds1.concatenate(train_ds2)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000, seed=226)\n",
    "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#\n",
    "t5 = np.array([np.expand_dims(item, axis=-1) for item in xtest1[0]])  # Expand dims\n",
    "t5 = t5.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t6 = np.array([item for item in xtest1[1]])  # NPY input\n",
    "t6 = scaler.transform(t6)  # Sử dụng scaler đã fit từ dữ liệu training\n",
    "test_ds1 = tf.data.Dataset.from_tensor_slices(((t5, t6), ytest1))\n",
    "\n",
    "#\n",
    "t7 = np.array([np.expand_dims(item, axis=-1) for item in xtest2[0]])  # Expand dims\n",
    "t7 = t7.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t8 = np.array([item for item in xtest2[1]])  # NPY input\n",
    "t8 = scaler.transform(t8)  # Sử dụng scaler đã fit từ dữ liệu training\n",
    "test_ds2 = tf.data.Dataset.from_tensor_slices(((t7, t8), ytest2))\n",
    "\n",
    "#\n",
    "test_ds = test_ds1.concatenate(test_ds2)\n",
    "test_ds = test_ds.shuffle(buffer_size=1000, seed=226)\n",
    "test_ds = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenConcatenate(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to concatenate tensors with trainable=False.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FrozenConcatenate, self).__init__(**kwargs)\n",
    "        self.trainable = False  # Set trainable=False explicitly\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.concat(inputs, axis=-1)\n",
    "\n",
    "def create_downstream_task_model(freeze=True):\n",
    "    # Load pretrained sub-networks\n",
    "    image_network = network_1()\n",
    "    joint_network = network_2()\n",
    "\n",
    "    # Freeze hoặc unfreeze các lớp của pretrained models\n",
    "    if freeze:\n",
    "        image_network.trainable = False\n",
    "        joint_network.trainable = False\n",
    "    else:\n",
    "        image_network.trainable = True\n",
    "        joint_network.trainable = True\n",
    "\n",
    "    # Input layers\n",
    "    image_input = layers.Input(shape=(128, 128, 1), name=\"image_input\")\n",
    "    joint_input = layers.Input(shape=(8,), name=\"joint_input\")\n",
    "\n",
    "    # Feature extraction\n",
    "    image_embedding = image_network(image_input)\n",
    "    joint_embedding = joint_network(joint_input)\n",
    "\n",
    "    # Concatenate embeddings using custom frozen layer\n",
    "    combined_embedding = FrozenConcatenate(name=\"frozen_concatenate\")([image_embedding, joint_embedding])\n",
    "\n",
    "    # Add a classification head\n",
    "    output = layers.Dense(2, activation=\"softmax\", name=\"classification\")(combined_embedding)\n",
    "\n",
    "    # Create model\n",
    "    downstream_model = tf.keras.Model(inputs=[image_input, joint_input], outputs=output, name=\"DownstreamTaskModel\")\n",
    "    \n",
    "    return downstream_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\code\\barlow\\env\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Trọng số lớp cuối network_1 trong downstream model:\n",
      "[array([[ 1.7791046e-03,  1.6006832e-03, -2.6084578e-03, ...,\n",
      "         2.1543987e-03,  1.8573399e-03,  8.8305678e-05],\n",
      "       [-4.1144942e-03, -3.5310960e-03,  4.3727746e-03, ...,\n",
      "         2.3173885e-03,  1.1221957e-03,  4.1901181e-03],\n",
      "       [-3.5800021e-03, -1.7465944e-03,  2.8693588e-03, ...,\n",
      "        -1.4517049e-03, -1.2668660e-03,  4.7524245e-03],\n",
      "       ...,\n",
      "       [-1.8497983e-03,  1.4876155e-03, -4.2687636e-05, ...,\n",
      "         3.4739599e-03, -1.8873890e-03, -4.6235183e-04],\n",
      "       [-2.1921620e-03,  9.4010960e-04, -1.6554489e-03, ...,\n",
      "         3.0351132e-03,  1.1770888e-03, -6.0964189e-04],\n",
      "       [ 3.6701662e-03, -3.7156683e-03, -4.5210710e-03, ...,\n",
      "         7.6347264e-04,  1.8666014e-03,  2.9561697e-03]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "Trọng số lớp cuối network_2 trong downstream model:\n",
      "[array([[-0.5244291 , -0.15840563,  0.5105635 ,  0.06519359,  0.32717806,\n",
      "        -0.07964972, -0.55921984,  0.050569  ,  0.540648  ,  0.05968094],\n",
      "       [-0.00418484,  0.2870996 ,  0.2020008 ,  0.35298902,  0.1740731 ,\n",
      "        -0.25690898,  0.2002123 , -0.16978565, -0.1520569 , -0.30224863],\n",
      "       [ 0.20883393,  0.2966348 ,  0.10735333,  0.40392178, -0.57490283,\n",
      "         0.2522481 ,  0.28995365,  0.4317491 , -0.46383882,  0.20901549],\n",
      "       [-0.06235152, -0.36498678,  0.22902155, -0.16768718,  0.54045165,\n",
      "         0.4755932 , -0.43528944,  0.26016802,  0.34672916, -0.30843163],\n",
      "       [ 0.25938642,  0.0448131 , -0.00274074,  0.25504935, -0.5066662 ,\n",
      "         0.37808454,  0.06503832,  0.2787425 ,  0.31641495,  0.5364356 ],\n",
      "       [-0.05152786,  0.04319847, -0.31175384,  0.45392013, -0.43212622,\n",
      "         0.22198707, -0.45377776,  0.5671294 ,  0.05047238,  0.06206357],\n",
      "       [ 0.46504092, -0.12381172, -0.09150228,  0.5412319 ,  0.41543204,\n",
      "        -0.48002562,  0.01467335, -0.4929363 ,  0.46297395, -0.5033902 ],\n",
      "       [-0.14835465,  0.46046436, -0.543642  ,  0.5530536 , -0.04205942,\n",
      "         0.44401097, -0.17836049,  0.06180447, -0.03534055,  0.23437113]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "Layer image_input trainable: True\n",
      "Layer joint_input trainable: True\n",
      "Layer ImageNetwork trainable: False\n",
      "Layer JointStateNetwork trainable: False\n",
      "Layer frozen_concatenate trainable: False\n",
      "Layer classification trainable: True\n"
     ]
    }
   ],
   "source": [
    "# Sau khi tạo downstream model\n",
    "downstream_model = create_downstream_task_model(freeze=True)\n",
    "\n",
    "# Kiểm tra lại trọng số của các lớp cuối cùng trong downstream task\n",
    "print(\"Trọng số lớp cuối network_1 trong downstream model:\")\n",
    "print(downstream_model.get_layer(\"ImageNetwork\").layers[-1].get_weights())\n",
    "\n",
    "print(\"Trọng số lớp cuối network_2 trong downstream model:\")\n",
    "print(downstream_model.get_layer(\"JointStateNetwork\").layers[-1].get_weights())\n",
    "\n",
    "\n",
    "for layer in downstream_model.layers:\n",
    "    print(f\"Layer {layer.name} trainable: {layer.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\barlow\\env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['image_input', 'joint_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5158 - loss: 0.6980 - val_accuracy: 0.4937 - val_loss: 0.7020\n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5030 - loss: 0.7013 - val_accuracy: 0.4937 - val_loss: 0.7019\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5366 - loss: 0.6934 - val_accuracy: 0.4937 - val_loss: 0.7018\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5116 - loss: 0.7012 - val_accuracy: 0.5000 - val_loss: 0.7018\n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5437 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.7017\n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5120 - loss: 0.6973 - val_accuracy: 0.5000 - val_loss: 0.7017\n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5157 - loss: 0.6998 - val_accuracy: 0.5000 - val_loss: 0.7016\n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5202 - loss: 0.6986 - val_accuracy: 0.5000 - val_loss: 0.7016\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4864 - loss: 0.7035 - val_accuracy: 0.5000 - val_loss: 0.7016\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5328 - loss: 0.6977 - val_accuracy: 0.5000 - val_loss: 0.7015\n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5067 - loss: 0.7026 - val_accuracy: 0.4937 - val_loss: 0.7015\n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5440 - loss: 0.6938 - val_accuracy: 0.4937 - val_loss: 0.7014\n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5494 - loss: 0.6911 - val_accuracy: 0.5000 - val_loss: 0.7014\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5342 - loss: 0.6925 - val_accuracy: 0.5000 - val_loss: 0.7014\n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4884 - loss: 0.7015 - val_accuracy: 0.5000 - val_loss: 0.7013\n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5467 - loss: 0.6871 - val_accuracy: 0.5000 - val_loss: 0.7013\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5155 - loss: 0.7001 - val_accuracy: 0.5000 - val_loss: 0.7013\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5203 - loss: 0.6983 - val_accuracy: 0.5063 - val_loss: 0.7012\n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5158 - loss: 0.6947 - val_accuracy: 0.5063 - val_loss: 0.7012\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5231 - loss: 0.6960 - val_accuracy: 0.5000 - val_loss: 0.7012\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "downstream_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = downstream_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Sample 0 --- Result:[0.60739094 0.39260906]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 1 --- Result:[0.4211709 0.5788291]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 2 --- Result:[0.38916326 0.6108368 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 3 --- Result:[0.41653994 0.58346003]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 4 --- Result:[0.46937034 0.53062963]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 5 --- Result:[0.53009886 0.4699011 ]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 6 --- Result:[0.60917145 0.39082852]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 7 --- Result:[0.4593875  0.54061246]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 8 --- Result:[0.525556 0.474444]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 9 --- Result:[0.35477707 0.64522296]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 10 --- Result:[0.3671954 0.6328046]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 11 --- Result:[0.50526565 0.49473438]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 12 --- Result:[0.50674623 0.4932538 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 13 --- Result:[0.45561776 0.5443822 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 14 --- Result:[0.5992065  0.40079352]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 15 --- Result:[0.52617586 0.47382408]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 16 --- Result:[0.42488748 0.57511246]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 17 --- Result:[0.4418471 0.558153 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 18 --- Result:[0.44945902 0.55054104]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 19 --- Result:[0.4683408 0.5316591]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 20 --- Result:[0.4223282 0.5776718]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 21 --- Result:[0.5425333  0.45746675]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 22 --- Result:[0.5187816 0.4812184]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 23 --- Result:[0.5754375 0.4245625]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 24 --- Result:[0.47513124 0.5248688 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 25 --- Result:[0.5722303  0.42776972]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 26 --- Result:[0.3545165 0.6454835]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 27 --- Result:[0.4898391  0.51016086]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 28 --- Result:[0.53164107 0.4683589 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 29 --- Result:[0.36226505 0.63773495]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 30 --- Result:[0.5297237  0.47027627]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 31 --- Result:[0.3347405 0.6652595]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 32 --- Result:[0.5481943 0.4518057]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 33 --- Result:[0.5061842  0.49381578]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 34 --- Result:[0.43905985 0.56094015]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 35 --- Result:[0.50285846 0.49714157]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 36 --- Result:[0.495455 0.504545]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 37 --- Result:[0.4509423 0.5490577]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 38 --- Result:[0.530305   0.46969497]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 39 --- Result:[0.61143184 0.3885682 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 40 --- Result:[0.5446191  0.45538092]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 41 --- Result:[0.38464642 0.6153536 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 42 --- Result:[0.47145018 0.52854985]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 43 --- Result:[0.41557875 0.5844213 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 44 --- Result:[0.5633006 0.4366994]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 45 --- Result:[0.44542825 0.55457175]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 46 --- Result:[0.39481938 0.6051806 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 47 --- Result:[0.32966724 0.67033273]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 48 --- Result:[0.37405032 0.6259497 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 49 --- Result:[0.5449901  0.45500985]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 50 --- Result:[0.4662923  0.53370774]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 51 --- Result:[0.4569426 0.5430573]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 52 --- Result:[0.5065728  0.49342722]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 53 --- Result:[0.5960636  0.40393645]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 54 --- Result:[0.53384864 0.4661514 ]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 55 --- Result:[0.38292557 0.61707443]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 56 --- Result:[0.5960299  0.40397015]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 57 --- Result:[0.56716913 0.43283084]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 58 --- Result:[0.59547037 0.40452957]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 59 --- Result:[0.41522983 0.58477014]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 60 --- Result:[0.32869142 0.6713086 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 61 --- Result:[0.5906125  0.40938753]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 62 --- Result:[0.56170213 0.4382979 ]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 63 --- Result:[0.5356429  0.46435714]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 64 --- Result:[0.36799696 0.63200307]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 65 --- Result:[0.56409806 0.43590194]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 66 --- Result:[0.41912708 0.58087283]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 67 --- Result:[0.57516176 0.42483822]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 68 --- Result:[0.44861695 0.5513831 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 69 --- Result:[0.50228757 0.49771243]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 70 --- Result:[0.5719051  0.42809483]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 71 --- Result:[0.53702253 0.46297744]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 72 --- Result:[0.53968465 0.4603154 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 73 --- Result:[0.3526325 0.6473675]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 74 --- Result:[0.45604643 0.54395354]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 75 --- Result:[0.60902065 0.3909793 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 76 --- Result:[0.5679842  0.43201572]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 77 --- Result:[0.45786372 0.54213625]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 78 --- Result:[0.4765187 0.5234813]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 79 --- Result:[0.5191627  0.48083732]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 80 --- Result:[0.51256305 0.48743698]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 81 --- Result:[0.51154804 0.488452  ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 82 --- Result:[0.48499823 0.5150018 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 83 --- Result:[0.46059534 0.5394047 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 84 --- Result:[0.50317395 0.49682608]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 85 --- Result:[0.54220295 0.4577971 ]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 86 --- Result:[0.54368883 0.45631108]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 87 --- Result:[0.5049231 0.4950769]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 88 --- Result:[0.5649773 0.4350227]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 89 --- Result:[0.4412058  0.55879414]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 90 --- Result:[0.49988085 0.50011915]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 91 --- Result:[0.58934236 0.41065767]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 92 --- Result:[0.49782452 0.50217545]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 93 --- Result:[0.3732636  0.62673646]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 94 --- Result:[0.45731026 0.54268974]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 95 --- Result:[0.400704   0.59929603]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 96 --- Result:[0.59582365 0.40417638]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 97 --- Result:[0.43206158 0.5679384 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 98 --- Result:[0.43539944 0.5646005 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 99 --- Result:[0.6056162  0.39438382]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 100 --- Result:[0.59251994 0.4074801 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 101 --- Result:[0.4077263 0.5922737]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 102 --- Result:[0.38876793 0.61123204]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 103 --- Result:[0.39827707 0.60172296]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 104 --- Result:[0.4293594 0.5706406]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 105 --- Result:[0.5301852  0.46981472]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 106 --- Result:[0.422171 0.577829]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 107 --- Result:[0.5102062  0.48979372]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 108 --- Result:[0.4652283 0.5347717]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 109 --- Result:[0.45880765 0.54119235]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 110 --- Result:[0.5331949  0.46680516]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 111 --- Result:[0.49504083 0.50495917]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 112 --- Result:[0.568081   0.43191898]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 113 --- Result:[0.5054065  0.49459347]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 114 --- Result:[0.5492954  0.45070457]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 115 --- Result:[0.61206347 0.38793656]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 116 --- Result:[0.36444455 0.6355555 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 117 --- Result:[0.42278904 0.57721096]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 118 --- Result:[0.56099385 0.43900615]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 119 --- Result:[0.5366719 0.4633281]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 120 --- Result:[0.38909298 0.610907  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 121 --- Result:[0.45673233 0.54326767]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 122 --- Result:[0.47224563 0.52775437]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 123 --- Result:[0.4705996 0.5294004]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 124 --- Result:[0.5069577  0.49304232]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 125 --- Result:[0.50087017 0.49912977]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 126 --- Result:[0.48283505 0.51716495]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 127 --- Result:[0.38935536 0.6106447 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 128 --- Result:[0.56918925 0.43081072]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 129 --- Result:[0.5000038  0.49999616]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 130 --- Result:[0.49690008 0.50309986]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 131 --- Result:[0.36397824 0.63602173]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 132 --- Result:[0.4172988 0.5827012]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 133 --- Result:[0.4096222 0.5903778]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 134 --- Result:[0.50521356 0.49478647]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 135 --- Result:[0.4211503 0.5788497]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 136 --- Result:[0.4036377  0.59636235]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 137 --- Result:[0.42447877 0.5755212 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 138 --- Result:[0.37042704 0.629573  ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 139 --- Result:[0.51692075 0.48307925]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 140 --- Result:[0.5699011 0.4300989]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 141 --- Result:[0.56382334 0.43617663]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 142 --- Result:[0.45320165 0.54679835]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 143 --- Result:[0.50926733 0.49073264]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 144 --- Result:[0.556579 0.443421]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 145 --- Result:[0.5111197  0.48888028]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 146 --- Result:[0.49856937 0.50143063]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 147 --- Result:[0.6241117  0.37588835]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 148 --- Result:[0.5544019 0.4455981]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 149 --- Result:[0.6115697  0.38843033]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 150 --- Result:[0.53731513 0.4626848 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 151 --- Result:[0.50770396 0.492296  ]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 152 --- Result:[0.5669572  0.43304276]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 153 --- Result:[0.50950384 0.49049613]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 154 --- Result:[0.51099706 0.48900303]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 155 --- Result:[0.54497826 0.4550217 ]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 156 --- Result:[0.38592598 0.614074  ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 157 --- Result:[0.41254032 0.5874597 ]\n",
      "Predicted Label = 1, True Label = 1\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "# Predict on the test dataset\n",
    "predictions = downstream_model.predict(test_ds)\n",
    "# Extract the true labels from the test dataset\n",
    "true_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# Print predictions and true labels\n",
    "for i, prediction in enumerate(predictions):\n",
    "    predicted_label = np.argmax(prediction)  # Get the predicted class index\n",
    "    print(f\"Sample {i} --- Result:{prediction}\")\n",
    "    print(f\"Predicted Label = {predicted_label}, True Label = {true_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
