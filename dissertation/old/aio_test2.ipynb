{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "#from func import *\n",
    "import func\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "folder_path = \"F:/code/barlow/UR5\"\n",
    "file_pairs = func.load_file_pairs(folder_path)\n",
    "\n",
    "images = []\n",
    "npy_file = []\n",
    "for jpg_path, npy_path in file_pairs:\n",
    "    images.append(func.read_jpg_files(jpg_path))\n",
    "    npy_file.append(func.read_and_parse_npy_file(npy_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "CROP_TO = 32\n",
    "SEED = 42\n",
    "\n",
    "PROJECT_DIM = 2048\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "def concat_encoder():\n",
    "    # Input \n",
    "    image_input = layers.Input(shape=(128, 128, 1), name=\"image_input\")  \n",
    "    npy_input = layers.Input(shape=(8,), name=\"npy_input\") \n",
    "\n",
    "    # x1: CNN output shape (30)\n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x1 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x1 = layers.Flatten()(x1)\n",
    "    x1 = layers.Dense(20, activation='relu', name=\"cnn_output\")(x1)\n",
    "\n",
    "    # x2: read npy_file with shape (8)\n",
    "    x2 = layers.Dense(10, activation='relu', name=\"npy_output\")(npy_input)\n",
    "    # x3: x1 + x2\n",
    "    x3 = layers.Concatenate(name=\"combined_features\")([x1, x2])\n",
    "    \n",
    "    # # straight npy_file shape (8)\n",
    "    # x3 = layers.Concatenate(name=\"combined_features\")([x1, npy_input])\n",
    "\n",
    "    # Fully connected layers\n",
    "    # x = layers.Dense(128, activation=\"relu\", name=\"dense_128\")(x3)\n",
    "    # x = layers.Dense(64, activation=\"relu\", name=\"dense_64\")(x)\n",
    "    # x = layers.Dense(32, activation=None, name=\"projection_head\")(x)  # Projection head\n",
    "\n",
    "    x = layers.Dense(32, activation=None, name=\"projection_head\")(x3)  # Projection head\n",
    "    # Model\n",
    "    model = models.Model(inputs=[image_input, npy_input], outputs=x, name=\"encoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concat_encoder():\n",
    "#     # Input \n",
    "#     image_input = layers.Input(shape=(128, 128, 1), name=\"image_input\")  \n",
    "#     npy_input = layers.Input(shape=(8,), name=\"npy_input\") \n",
    "\n",
    "#     # Modify ResNet-50 for grayscale input\n",
    "#     resnet_base = ResNet50(weights=None, include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "#     # Preprocess the grayscale input to have 3 channels (simulating RGB)\n",
    "#     x1 = layers.Conv2D(3, (1, 1), padding='same', name=\"grayscale_to_rgb\")(image_input)\n",
    "#     x1 = preprocess_input(x1)\n",
    "\n",
    "#     # Pass through ResNet-50\n",
    "#     x1 = resnet_base(x1)\n",
    "#     x1 = layers.GlobalAveragePooling2D(name=\"gap_output\")(x1)\n",
    "#     #x1 = layers.Flatten(name=\"flatten_output\")(x1)\n",
    "#     x1 = layers.Dense(30, activation='relu', name=\"cnn_output\", kernel_initializer=GlorotUniform())(x1)\n",
    "\n",
    "\n",
    "#     # Concatenate with npy_input\n",
    "#     #x3 = layers.Concatenate(name=\"combined_features\")([x1, npy_input])\n",
    "\n",
    "#     #x2: read npy_file with shape ()\n",
    "#     x2 = layers.Dense(10, activation='relu', name=\"npy_output\")(npy_input)\n",
    "#     #x3: x1 + x2\n",
    "#     x3 = layers.Concatenate(name=\"combined_features\")([x1, x2])\n",
    "\n",
    "#     # Fully connected layers\n",
    "#     # x = layers.Dense(128, activation=\"relu\", name=\"dense_128\")(x3)\n",
    "#     # x = layers.Dense(64, activation=\"relu\", name=\"dense_64\")(x)\n",
    "#     x = layers.Dense(32, activation=None, name=\"projection_head\")(x3)  # Projection head\n",
    "\n",
    "#     # Model\n",
    "#     model = models.Model(inputs=[image_input, npy_input], outputs=x, name=\"encoder\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concat_encoder():\n",
    "#     # Input \n",
    "#     image_input = layers.Input(shape=(128, 128, 1), name=\"image_input\")  \n",
    "#     npy_input = layers.Input(shape=(8,), name=\"npy_input\") \n",
    "\n",
    "#     resnet_base = ResNet50(weights=None, include_top=False, input_shape=(128, 128, 3))\n",
    "#     x1 = layers.Conv2D(3, (1, 1), padding='same', name=\"grayscale_to_rgb\")(image_input)\n",
    "#     x1 = preprocess_input(x1)\n",
    "\n",
    "#     x1 = resnet_base(x1)\n",
    "#     x1 = layers.GlobalAveragePooling2D(name=\"gap_output\")(x1)\n",
    "    \n",
    "#     # Skip connection for Dense\n",
    "#     cnn_skip = x1  # Save ResNet-50 output before Dense\n",
    "#     x1 = layers.Dense(30, activation='relu', name=\"cnn_output\", kernel_initializer=GlorotUniform())(x1)\n",
    "#     x1 = layers.Add(name=\"cnn_skip_connection\")([cnn_skip, x1])  # Skip connection\n",
    "\n",
    "#     # Process npy_input\n",
    "#     x2 = layers.Dense(10, activation='relu', name=\"npy_output\")(npy_input)\n",
    "\n",
    "#     # Concatenate features\n",
    "#     x3 = layers.Concatenate(name=\"combined_features\")([x1, x2])\n",
    "\n",
    "#     # Fully connected layers\n",
    "#     x = layers.Dense(32, activation=None, name=\"projection_head\")(x3)  # Projection head\n",
    "\n",
    "#     # Model\n",
    "#     model = models.Model(inputs=[image_input, npy_input], outputs=x, name=\"encoder\")\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Functions\n",
    "# No1. Augmentation Functions\n",
    "# def augment_image(image):\n",
    "#     # \"\"\"\"\n",
    "#     # random flip + random crop\n",
    "#     # \"\"\"\n",
    "#     # if random.random() > 0.5:\n",
    "#     #     image = np.fliplr(image) # left right\n",
    "#     # if random.random() > 0.5:\n",
    "#     #     image = np.flipud(image) # up down\n",
    "    \n",
    "#     # crop_size = random.randint(5, 20)\n",
    "#     # h, w = image.shape\n",
    "#     # cropped_image = image[crop_size:h-crop_size, crop_size:w-crop_size]\n",
    "#     # # Resize lại về kích thước ban đầu\n",
    "#     # augmented_image = np.pad(cropped_image, ((crop_size, crop_size), (crop_size, crop_size)), mode='constant', constant_values=77)\n",
    "#     # return augmented_image\n",
    "#     return image\n",
    "\n",
    "\n",
    "def augment_image(image, crop_ratio=0.05):\n",
    "    h, w = image.shape\n",
    "    # Calculate crop dimensions\n",
    "    crop_h = int(h * crop_ratio)\n",
    "    crop_w = int(w * crop_ratio)\n",
    "\n",
    "    # cropping\n",
    "    cropped_image = image[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "\n",
    "    # Resize back to original\n",
    "    resized_image = cv2.resize(cropped_image, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "# No2. Augmentation Functions\n",
    "def adjust_brightness(image, factor=1.1):\n",
    "    \"\"\"\n",
    "    scale pixel values\n",
    "    \"\"\"\n",
    "    adjusted = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
    "    return adjusted\n",
    "\n",
    "def add_noise_vector(vector,alpha= 0.01):\n",
    "    noise = np.random.normal(0, alpha, size=len(vector)) # alpha standard deviation\n",
    "    augmented_vector = [v + n for v, n in zip(vector, noise)]\n",
    "    #augmented_vector = [i.astype(float) for i in augmented_vector]\n",
    "    return augmented_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "STEPS_PER_EPOCH = 597 // BATCH_SIZE\n",
    "TOTAL_STEPS = STEPS_PER_EPOCH * EPOCHS\n",
    "WARMUP_EPOCHS = int(EPOCHS * 0.1)\n",
    "WARMUP_STEPS = int(WARMUP_EPOCHS * STEPS_PER_EPOCH)\n",
    "\n",
    "lr_decayed_fn = lr_scheduler.WarmUpCosine(\n",
    "    learning_rate_base=5e-4,\n",
    "    total_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=WARMUP_STEPS\n",
    ")\n",
    "\n",
    "#lr_decayed_fn = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train loss data\n",
    "# image\n",
    "image1 = [augment_image(img) for img in images]\n",
    "image2 = [adjust_brightness(img, factor=1.2) for img in images]\n",
    "\n",
    "# noise\n",
    "npy_file_1 = [add_noise_vector(i,alpha= 0.05) for i in npy_file]\n",
    "npy_file_2 = [add_noise_vector(i,alpha= 0.10) for i in npy_file]\n",
    "\n",
    "# Normalize\n",
    "image1 = [(i.astype(\"float32\") / 255.0) for i in image1]\n",
    "image2 = [(i.astype(\"float32\") / 255.0) for i in image2]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "npy_file_1 = scaler.fit_transform(npy_file_1)\n",
    "npy_file_2 = scaler.fit_transform(npy_file_2)\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices((image1, npy_file_1)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((image2, npy_file_2)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "BL_ds = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "BL_ds_test = BL_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\barlow\\env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['image_input', 'npy_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - loss: 8.6701\n",
      "Epoch 2/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 365ms/step - loss: 3.7775\n",
      "Epoch 3/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 357ms/step - loss: 2.0396\n",
      "Epoch 4/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 349ms/step - loss: 2.0618\n",
      "Epoch 5/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - loss: 1.9682\n",
      "Epoch 6/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - loss: 1.8743\n",
      "Epoch 7/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - loss: 1.8375\n",
      "Epoch 8/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 358ms/step - loss: 1.8051\n",
      "Epoch 9/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - loss: 1.7967\n",
      "Epoch 10/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 342ms/step - loss: 1.7907\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Barlow Twins\n",
    "encoder = concat_encoder()\n",
    "barlow_twins = func.BarlowTwins(encoder=encoder, lambd=5e-3)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9)\n",
    "barlow_twins.compile(optimizer=optimizer)\n",
    "history = barlow_twins.fit(BL_ds_test, epochs=EPOCHS)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"Barlow Twin Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "#barlow_twins.save_weights('bl_test2.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func\n",
    "def process_data(data_list):\n",
    "    a = []\n",
    "    b = []\n",
    "    for jpg_path, npy_path in data_list:\n",
    "        a.append(func.read_jpg_files(jpg_path))\n",
    "        b.append(func.read_and_parse_npy_file(npy_path))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test dataset\n",
    "train_similar, train_dissimilar, test_similar, test_dissimilar = func.split_and_shuffle_pairs(file_pairs, folder_path)\n",
    "\n",
    "xtrain1 = process_data(train_similar)\n",
    "xtrain2 = process_data(train_dissimilar)\n",
    "xtest1 = process_data(test_similar)\n",
    "xtest2 = process_data(test_dissimilar)\n",
    "\n",
    "# label 1 - similar, 0 - disimilar\n",
    "ytrain1 = [1] * 500\n",
    "ytrain2 = [0] * 500\n",
    "ytest1 = [1] *79\n",
    "ytest2 = [0] *79\n",
    "\n",
    "#\n",
    "t1 = np.array([np.expand_dims(item, axis=-1) for item in xtrain1[0]])  # Expand dims\n",
    "t1 = t1.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t2 = np.array([item for item in xtrain1[1]])  # NPY input\n",
    "t2 = scaler.fit_transform(t2)  # Standardize to mean=0, std=1\n",
    "train_ds1 = tf.data.Dataset.from_tensor_slices(((t1, t2), ytrain1))\n",
    "\n",
    "#\n",
    "t3 = np.array([np.expand_dims(item, axis=-1) for item in xtrain2[0]])  # Expand dims\n",
    "t3 = t3.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t4 = np.array([item for item in xtrain2[1]])  # NPY input\n",
    "t4 = scaler.transform(t4)  # Sử dụng scaler đã fit từ trước\n",
    "train_ds2 = tf.data.Dataset.from_tensor_slices(((t3, t4), ytrain2))\n",
    "\n",
    "\n",
    "# concat\n",
    "train_ds = train_ds1.concatenate(train_ds2)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000, seed=226)\n",
    "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#\n",
    "t5 = np.array([np.expand_dims(item, axis=-1) for item in xtest1[0]])  # Expand dims\n",
    "t5 = t5.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t6 = np.array([item for item in xtest1[1]])  # NPY input\n",
    "t6 = scaler.transform(t6)  # Sử dụng scaler đã fit từ dữ liệu training\n",
    "test_ds1 = tf.data.Dataset.from_tensor_slices(((t5, t6), ytest1))\n",
    "\n",
    "#\n",
    "t7 = np.array([np.expand_dims(item, axis=-1) for item in xtest2[0]])  # Expand dims\n",
    "t7 = t7.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "t8 = np.array([item for item in xtest2[1]])  # NPY input\n",
    "t8 = scaler.transform(t8)  # Sử dụng scaler đã fit từ dữ liệu training\n",
    "test_ds2 = tf.data.Dataset.from_tensor_slices(((t7, t8), ytest2))\n",
    "\n",
    "#\n",
    "test_ds = test_ds1.concatenate(test_ds2)\n",
    "test_ds = test_ds.shuffle(buffer_size=1000, seed=226)\n",
    "test_ds = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create encoder \n",
    "# encoder = concat_encoder()\n",
    "# barlow_twins = func.BarlowTwins(encoder=encoder)\n",
    "\n",
    "\n",
    "backbone = tf.keras.Model(\n",
    "    inputs=barlow_twins.encoder.input,\n",
    "    outputs=barlow_twins.encoder.layers[-1].output  # output từ lớp Dense(16)\n",
    ")\n",
    "# Freeze \n",
    "#barlow_twins.load_weights('bl_test2.weights.h5')\n",
    "backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.5114 - loss: 3.8510 - val_accuracy: 0.4747 - val_loss: 2.4343\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.4971 - loss: 2.4602 - val_accuracy: 0.4747 - val_loss: 0.8268\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5046 - loss: 0.7215 - val_accuracy: 0.4747 - val_loss: 0.7240\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5157 - loss: 0.7037 - val_accuracy: 0.4747 - val_loss: 0.7162\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.4902 - loss: 0.7025 - val_accuracy: 0.4747 - val_loss: 0.7120\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5008 - loss: 0.6910 - val_accuracy: 0.4747 - val_loss: 0.7089\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5102 - loss: 0.6920 - val_accuracy: 0.4747 - val_loss: 0.7069\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5063 - loss: 0.6969 - val_accuracy: 0.4747 - val_loss: 0.7051\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5090 - loss: 0.6938 - val_accuracy: 0.4747 - val_loss: 0.7038\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5088 - loss: 0.6987 - val_accuracy: 0.4747 - val_loss: 0.7026\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5138 - loss: 0.6884 - val_accuracy: 0.4747 - val_loss: 0.7020\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5149 - loss: 0.6866 - val_accuracy: 0.4747 - val_loss: 0.7013\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5054 - loss: 0.6930 - val_accuracy: 0.4747 - val_loss: 0.7008\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4963 - loss: 0.6975 - val_accuracy: 0.4747 - val_loss: 0.7001\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5027 - loss: 0.6874 - val_accuracy: 0.4747 - val_loss: 0.6998\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4938 - loss: 0.6930 - val_accuracy: 0.4747 - val_loss: 0.6995\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5016 - loss: 0.6933 - val_accuracy: 0.4747 - val_loss: 0.6992\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4935 - loss: 0.6931 - val_accuracy: 0.4747 - val_loss: 0.6989\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4775 - loss: 0.6945 - val_accuracy: 0.4747 - val_loss: 0.6988\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4942 - loss: 0.6972 - val_accuracy: 0.4747 - val_loss: 0.6985\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4928 - loss: 0.6937 - val_accuracy: 0.4747 - val_loss: 0.6983\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4868 - loss: 0.6871 - val_accuracy: 0.4747 - val_loss: 0.6980\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.5185 - loss: 0.6903 - val_accuracy: 0.4747 - val_loss: 0.6979\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4887 - loss: 0.6912 - val_accuracy: 0.4747 - val_loss: 0.6978\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4800 - loss: 0.6879 - val_accuracy: 0.4747 - val_loss: 0.6977\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4912 - loss: 0.6948 - val_accuracy: 0.4747 - val_loss: 0.6975\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5205 - loss: 0.6902 - val_accuracy: 0.4747 - val_loss: 0.6974\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4988 - loss: 0.6880 - val_accuracy: 0.4747 - val_loss: 0.6973\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5132 - loss: 0.6885 - val_accuracy: 0.4747 - val_loss: 0.6971\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4755 - loss: 0.6895 - val_accuracy: 0.4747 - val_loss: 0.6970\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5011 - loss: 0.6874 - val_accuracy: 0.4747 - val_loss: 0.6969\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4985 - loss: 0.6878 - val_accuracy: 0.4747 - val_loss: 0.6969\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5098 - loss: 0.6895 - val_accuracy: 0.4747 - val_loss: 0.6969\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5032 - loss: 0.6918 - val_accuracy: 0.4747 - val_loss: 0.6967\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4944 - loss: 0.6892 - val_accuracy: 0.4747 - val_loss: 0.6967\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5239 - loss: 0.6858 - val_accuracy: 0.4747 - val_loss: 0.6967\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5116 - loss: 0.6939 - val_accuracy: 0.4747 - val_loss: 0.6967\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5227 - loss: 0.6876 - val_accuracy: 0.4747 - val_loss: 0.6966\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5104 - loss: 0.6878 - val_accuracy: 0.4747 - val_loss: 0.6966\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5094 - loss: 0.6847 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5014 - loss: 0.6885 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4875 - loss: 0.6899 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4954 - loss: 0.6875 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5013 - loss: 0.6946 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5074 - loss: 0.6871 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5173 - loss: 0.6888 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4939 - loss: 0.6900 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5256 - loss: 0.6924 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4975 - loss: 0.6953 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4874 - loss: 0.6883 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5010 - loss: 0.6879 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4891 - loss: 0.6930 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5047 - loss: 0.6846 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4796 - loss: 0.6923 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5352 - loss: 0.6857 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4940 - loss: 0.6906 - val_accuracy: 0.4747 - val_loss: 0.6966\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4969 - loss: 0.6879 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4977 - loss: 0.6866 - val_accuracy: 0.4747 - val_loss: 0.6965\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4867 - loss: 0.6884 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4935 - loss: 0.6920 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4970 - loss: 0.6877 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4901 - loss: 0.6935 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4802 - loss: 0.6873 - val_accuracy: 0.4747 - val_loss: 0.6964\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4991 - loss: 0.6859 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4901 - loss: 0.6880 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4761 - loss: 0.6884 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4888 - loss: 0.6891 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5097 - loss: 0.6875 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 69/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4844 - loss: 0.6883 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4885 - loss: 0.6921 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5055 - loss: 0.6885 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5019 - loss: 0.6873 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4701 - loss: 0.6915 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5025 - loss: 0.6857 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5097 - loss: 0.6876 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4880 - loss: 0.6901 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.5135 - loss: 0.6852 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4760 - loss: 0.6922 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5152 - loss: 0.6875 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5024 - loss: 0.6894 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5130 - loss: 0.6893 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.5128 - loss: 0.6830 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.4945 - loss: 0.6907 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4978 - loss: 0.6880 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.5175 - loss: 0.6864 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.5020 - loss: 0.6882 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.5117 - loss: 0.6885 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.4938 - loss: 0.6871 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4794 - loss: 0.6900 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.5284 - loss: 0.6911 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.5006 - loss: 0.6885 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4766 - loss: 0.6960 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4682 - loss: 0.6893 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5015 - loss: 0.6923 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4877 - loss: 0.6906 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.4847 - loss: 0.6897 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5069 - loss: 0.6866 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4921 - loss: 0.6898 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5009 - loss: 0.6867 - val_accuracy: 0.4747 - val_loss: 0.6963\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.5015 - loss: 0.6840 - val_accuracy: 0.4747 - val_loss: 0.6962\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5081 - loss: 0.6942\n",
      "Test accuracy: 47.47%\n"
     ]
    }
   ],
   "source": [
    "# add classify layer\n",
    "inputs = backbone.input\n",
    "x = backbone.output\n",
    "outputs = layers.Dense(2, activation=\"relu\", name=\"classifier\")(x)\n",
    "\n",
    "# Classify model\n",
    "test_model = tf.keras.Model(inputs, outputs, name=\"test_model\")\n",
    "\n",
    "# Compile model\n",
    "test_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.00002)\n",
    ")\n",
    "\n",
    "history = test_model.fit(\n",
    "    train_ds, validation_data=test_ds, epochs=100\n",
    ")\n",
    "\n",
    "_, test_acc = test_model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C36F55A9E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C36F55A9E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Sample 0 --- Result:[2.2045531 4.582402 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 1 --- Result:[4.978539  1.7666178]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 2 --- Result:[1.4480395 3.0278285]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 3 --- Result:[2.233521  4.2971215]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 4 --- Result:[1.1397116 3.228251 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 5 --- Result:[1.1106937 3.3730915]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 6 --- Result:[0.7140896 3.5710313]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 7 --- Result:[1.9554253  0.69000137]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 8 --- Result:[0.5622608 3.5751348]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 9 --- Result:[2.512566 4.773095]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 10 --- Result:[0.79726094 3.3469644 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 11 --- Result:[2.532078 4.238161]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 12 --- Result:[1.4228258 3.1967387]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 13 --- Result:[0.90184873 3.1548038 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 14 --- Result:[1.9872262 3.0881734]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 15 --- Result:[0.6576225 3.237815 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 16 --- Result:[2.3058228 4.046164 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 17 --- Result:[2.48277  4.151287]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 18 --- Result:[0.7226278 3.0785713]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 19 --- Result:[1.2707574 3.112505 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 20 --- Result:[2.2432606  0.36351335]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 21 --- Result:[0.7609386 3.383994 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 22 --- Result:[2.1640856 4.295102 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 23 --- Result:[2.7367678 4.5057774]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 24 --- Result:[1.2161651 3.3376603]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 25 --- Result:[2.1342936 3.3697145]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 26 --- Result:[0.6360039 3.4171243]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 27 --- Result:[5.2826786 2.8382964]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 28 --- Result:[2.4937308 4.703543 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 29 --- Result:[1.999619   0.46132606]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 30 --- Result:[2.3079877 4.4701405]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 31 --- Result:[2.5031886 4.648384 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 32 --- Result:[1.8614374 3.6203485]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 33 --- Result:[0.47725624 3.0028718 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 34 --- Result:[2.520719  4.0162964]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 35 --- Result:[2.4539409 4.215114 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 36 --- Result:[1.9445202 4.701437 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 37 --- Result:[2.0284348 3.801382 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 38 --- Result:[5.319887  2.6552682]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 39 --- Result:[2.5054543  0.86994183]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 40 --- Result:[2.1379676 3.974186 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 41 --- Result:[2.5505152 4.535083 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 42 --- Result:[2.4549522 4.291625 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 43 --- Result:[3.432527 1.969118]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 44 --- Result:[2.2261918  0.42456943]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 45 --- Result:[1.5910969 2.8422842]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 46 --- Result:[0.46065348 3.0202017 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 47 --- Result:[2.0542393 4.4693613]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 48 --- Result:[0.6862318 2.7549746]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 49 --- Result:[2.395588  4.6139126]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 50 --- Result:[0.39724392 2.7718892 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 51 --- Result:[2.6174731 4.3209596]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 52 --- Result:[5.507842  2.1657856]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 53 --- Result:[0.54969496 2.8457072 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 54 --- Result:[2.5380988 3.9252515]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 55 --- Result:[0.72839946 2.6848993 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 56 --- Result:[2.502923 4.235883]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 57 --- Result:[2.5540428 4.3557944]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 58 --- Result:[1.6426346 3.1101966]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 59 --- Result:[2.1227021 5.0481124]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 60 --- Result:[2.141817  3.5917745]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 61 --- Result:[1.0502214 3.2816124]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 62 --- Result:[1.5449736 2.932927 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 63 --- Result:[1.6766927 3.58511  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 64 --- Result:[0.60741776 3.527962  ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 65 --- Result:[0.72165745 2.6813407 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 66 --- Result:[1.9395108 4.3231153]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 67 --- Result:[2.4828782 0.8141056]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 68 --- Result:[2.6706543 4.226247 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 69 --- Result:[2.5033817 4.163049 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 70 --- Result:[1.0853646 3.6092525]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 71 --- Result:[0.7564706 2.8707223]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 72 --- Result:[0.5840096 3.8327708]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 73 --- Result:[2.1163933 4.4313116]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 74 --- Result:[2.2438781 3.6661553]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 75 --- Result:[1.7047396 3.5109744]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 76 --- Result:[0.5541708 3.4491234]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 77 --- Result:[0.56764024 3.8516467 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 78 --- Result:[1.4841595 2.9842257]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 79 --- Result:[1.1434436 2.9225154]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 80 --- Result:[0.64965314 3.5842488 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 81 --- Result:[0.55335206 3.5432286 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 82 --- Result:[0.5268871 2.990161 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 83 --- Result:[0.9143855 2.8174715]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 84 --- Result:[0.4568128 3.2918267]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 85 --- Result:[2.2518468 4.346615 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 86 --- Result:[0.75390214 2.80687   ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 87 --- Result:[2.3686244 3.7730422]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 88 --- Result:[0.6658873 3.6211855]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 89 --- Result:[2.5325966 4.473117 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 90 --- Result:[0.5735921 3.3015862]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 91 --- Result:[2.6730187 4.2498446]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 92 --- Result:[2.4046476 4.6097355]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 93 --- Result:[2.7301426 4.287077 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 94 --- Result:[0.80174845 4.2271523 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 95 --- Result:[0.8390717 2.7969613]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 96 --- Result:[2.4135342 4.3838606]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 97 --- Result:[1.446821  3.7124414]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 98 --- Result:[1.9327815 3.6627638]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 99 --- Result:[2.3649588 4.019025 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 100 --- Result:[1.8702521 4.2123847]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 101 --- Result:[1.0123181 3.6785192]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 102 --- Result:[1.2845657 3.0175378]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 103 --- Result:[6.564078  3.9891288]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 104 --- Result:[2.1470933 4.5089555]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 105 --- Result:[0.4650144 3.3903964]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 106 --- Result:[1.8851609 3.5953383]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 107 --- Result:[0.57894176 3.525597  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 108 --- Result:[1.5700977 3.301118 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 109 --- Result:[2.155342  4.2841015]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 110 --- Result:[2.388173  4.4391727]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 111 --- Result:[2.5085065 4.2753935]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 112 --- Result:[0.59878486 3.605025  ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 113 --- Result:[2.2133331  0.44470087]\n",
      "Predicted Label = 0, True Label = 0\n",
      "Sample 114 --- Result:[0.7587642 2.846201 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 115 --- Result:[0.9184007 3.3095713]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 116 --- Result:[0.90467304 2.946433  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 117 --- Result:[0.52448267 3.1926177 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 118 --- Result:[2.054869  3.2062285]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 119 --- Result:[1.2419858 3.1166048]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 120 --- Result:[2.4275718 4.4885664]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 121 --- Result:[2.64297   4.1535807]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 122 --- Result:[1.0573907 3.0034797]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 123 --- Result:[2.6292453 4.357378 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 124 --- Result:[2.2482808 4.1608944]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 125 --- Result:[1.3145847 3.0682676]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 126 --- Result:[2.0630662 4.2416987]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 127 --- Result:[2.0994627 4.083336 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 128 --- Result:[0.6252927 2.8359752]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 129 --- Result:[0.6815168 3.3846626]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 130 --- Result:[2.4132295 4.676604 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 131 --- Result:[0.7615711 2.7738228]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 132 --- Result:[0.54438466 2.7896829 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 133 --- Result:[3.4541452 0.8921952]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 134 --- Result:[1.9757766 3.690755 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 135 --- Result:[2.430287  4.2862005]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 136 --- Result:[2.4918427 4.5721684]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 137 --- Result:[2.0503728 4.5348415]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 138 --- Result:[0.633125  2.8009458]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 139 --- Result:[2.346318 4.112736]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 140 --- Result:[0.6453683 2.88553  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 141 --- Result:[0.9712047 2.9381843]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 142 --- Result:[2.4877453 4.503109 ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 143 --- Result:[1.5842783 3.20297  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 144 --- Result:[2.2484179 4.6755137]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 145 --- Result:[2.6139092 4.9018087]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 146 --- Result:[3.9503975 1.3018739]\n",
      "Predicted Label = 0, True Label = 1\n",
      "Sample 147 --- Result:[1.2591047 3.0901005]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 148 --- Result:[0.35148162 3.2822871 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 149 --- Result:[1.154803  3.0506105]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 150 --- Result:[2.4853983 4.6863356]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 151 --- Result:[0.51339406 3.0070364 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 152 --- Result:[2.686668 4.415822]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 153 --- Result:[0.57371086 2.909712  ]\n",
      "Predicted Label = 1, True Label = 1\n",
      "Sample 154 --- Result:[0.74871033 3.1541874 ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 155 --- Result:[0.92416805 2.976559  ]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 156 --- Result:[1.2025268 3.0639803]\n",
      "Predicted Label = 1, True Label = 0\n",
      "Sample 157 --- Result:[2.2126808 0.3916314]\n",
      "Predicted Label = 0, True Label = 1\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "# Predict on the test dataset\n",
    "predictions = test_model.predict(test_ds)\n",
    "# Extract the true labels from the test dataset\n",
    "true_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# Print predictions and true labels\n",
    "for i, prediction in enumerate(predictions):\n",
    "    predicted_label = np.argmax(prediction)  # Get the predicted class index\n",
    "    print(f\"Sample {i} --- Result:{prediction}\")\n",
    "    print(f\"Predicted Label = {predicted_label}, True Label = {true_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
